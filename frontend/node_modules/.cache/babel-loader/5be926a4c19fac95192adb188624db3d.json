{"ast":null,"code":"/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\nimport { _FusedMatMul } from '@tensorflow/tfjs-core';\nimport { FusableActivation } from './types';\nvar wasmFusedMatMul;\n\nfunction setup(backend) {\n  wasmFusedMatMul = backend.wasm.cwrap(_FusedMatMul, null\n  /* void */\n  , ['number', 'array', 'number', 'number', 'array', 'number', 'number', 'number', 'number', 'number', 'number', 'number', 'number' // out_id\n  ]);\n}\n\nfunction fusedBatchMatMul(args) {\n  var inputs = args.inputs,\n      backend = args.backend,\n      attrs = args.attrs;\n  var a = inputs.a,\n      b = inputs.b,\n      bias = inputs.bias,\n      preluActivationWeights = inputs.preluActivationWeights;\n\n  if (a.dtype !== 'float32' || b.dtype !== 'float32') {\n    throw new Error(\"_FusedMatMul for non non-float32 tensors not yet supported.\");\n  }\n\n  var transposeA = attrs.transposeA,\n      transposeB = attrs.transposeB,\n      activation = attrs.activation,\n      leakyreluAlpha = attrs.leakyreluAlpha;\n  var aId = backend.dataIdMap.get(a.dataId).id;\n  var bId = backend.dataIdMap.get(b.dataId).id;\n  var biasId = 0;\n\n  if (bias != null) {\n    var biasData = backend.dataIdMap.get(bias.dataId);\n\n    if (biasData.shape.length !== 1) {\n      throw new Error(\"_FusedMatMul only supports rank-1 bias but got \" + \"rank \".concat(biasData.shape.length, \".\"));\n    }\n\n    biasId = biasData.id;\n  }\n\n  var preluActivationWeightsId = preluActivationWeights == null ? 0 : backend.dataIdMap.get(preluActivationWeights.dataId).id;\n  var fusedActivation = FusableActivation[activation];\n\n  if (fusedActivation == null) {\n    throw new Error(\"\".concat(activation, \" activation not yet supported for FusedConv2D \") + \"in the wasm backend.\");\n  }\n\n  var leftDim = transposeA ? a.shape[2] : a.shape[1];\n  var rightDim = transposeB ? b.shape[1] : b.shape[2];\n  var batchDim = a.shape[0];\n  var out = backend.makeOutput([batchDim, leftDim, rightDim], a.dtype);\n  var outId = backend.dataIdMap.get(out.dataId).id;\n  var aShapeBytes = new Uint8Array(new Int32Array(a.shape).buffer);\n  var bShapeBytes = new Uint8Array(new Int32Array(b.shape).buffer);\n  wasmFusedMatMul(aId, aShapeBytes, a.shape.length, bId, bShapeBytes, b.shape.length, transposeA, transposeB, fusedActivation, biasId, preluActivationWeightsId, leakyreluAlpha || 0, outId);\n  return out;\n}\n\nexport var fusedMatMulConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'wasm',\n  setupFunc: setup,\n  kernelFunc: fusedBatchMatMul\n};","map":{"version":3,"sources":["../../src/kernels/_FusedMatMul.ts"],"names":[],"mappings":"AAAA;;;;;;;;;;;;;;;AAeG;AAEH,SAAQ,YAAR,QAA4F,uBAA5F;AAIA,SAAQ,iBAAR,QAAgC,SAAhC;AAEA,IAAI,eAAJ;;AAOA,SAAS,KAAT,CAAe,OAAf,EAAmC;AACjC,EAAA,eAAe,GAAG,OAAO,CAAC,IAAR,CAAa,KAAb,CAAmB,YAAnB,EAAiC;AAAK;AAAtC,IAAkD,CAClE,QADkE,EAElE,OAFkE,EAGlE,QAHkE,EAIlE,QAJkE,EAKlE,OALkE,EAMlE,QANkE,EAOlE,QAPkE,EAQlE,QARkE,EASlE,QATkE,EAUlE,QAVkE,EAWlE,QAXkE,EAYlE,QAZkE,EAalE,QAbkE,CAavD;AAbuD,GAAlD,CAAlB;AAeD;;AAED,SAAS,gBAAT,CAA0B,IAA1B,EAIC;AAAA,MACQ,MADR,GACkC,IADlC,CACQ,MADR;AAAA,MACgB,OADhB,GACkC,IADlC,CACgB,OADhB;AAAA,MACyB,KADzB,GACkC,IADlC,CACyB,KADzB;AAAA,MAEQ,CAFR,GAE8C,MAF9C,CAEQ,CAFR;AAAA,MAEW,CAFX,GAE8C,MAF9C,CAEW,CAFX;AAAA,MAEc,IAFd,GAE8C,MAF9C,CAEc,IAFd;AAAA,MAEoB,sBAFpB,GAE8C,MAF9C,CAEoB,sBAFpB;;AAIC,MAAI,CAAC,CAAC,KAAF,KAAY,SAAZ,IAAyB,CAAC,CAAC,KAAF,KAAY,SAAzC,EAAoD;AAClD,UAAM,IAAI,KAAJ,+DAAN;AAED;;AAPF,MASQ,UATR,GAS8D,KAT9D,CASQ,UATR;AAAA,MASoB,UATpB,GAS8D,KAT9D,CASoB,UATpB;AAAA,MASgC,UAThC,GAS8D,KAT9D,CASgC,UAThC;AAAA,MAS4C,cAT5C,GAS8D,KAT9D,CAS4C,cAT5C;AAUC,MAAM,GAAG,GAAG,OAAO,CAAC,SAAR,CAAkB,GAAlB,CAAsB,CAAC,CAAC,MAAxB,EAAgC,EAA5C;AACA,MAAM,GAAG,GAAG,OAAO,CAAC,SAAR,CAAkB,GAAlB,CAAsB,CAAC,CAAC,MAAxB,EAAgC,EAA5C;AAEA,MAAI,MAAM,GAAG,CAAb;;AACA,MAAI,IAAI,IAAI,IAAZ,EAAkB;AAChB,QAAM,QAAQ,GAAG,OAAO,CAAC,SAAR,CAAkB,GAAlB,CAAsB,IAAI,CAAC,MAA3B,CAAjB;;AACA,QAAI,QAAQ,CAAC,KAAT,CAAe,MAAf,KAA0B,CAA9B,EAAiC;AAC/B,YAAM,IAAI,KAAJ,CACF,mEACQ,QAAQ,CAAC,KAAT,CAAe,MADvB,MADE,CAAN;AAGD;;AACD,IAAA,MAAM,GAAG,QAAQ,CAAC,EAAlB;AACD;;AACD,MAAM,wBAAwB,GAAG,sBAAsB,IAAI,IAA1B,GAC7B,CAD6B,GAE7B,OAAO,CAAC,SAAR,CAAkB,GAAlB,CAAsB,sBAAsB,CAAC,MAA7C,EAAqD,EAFzD;AAGA,MAAM,eAAe,GACjB,iBAAiB,CAAC,UAAD,CADrB;;AAEA,MAAI,eAAe,IAAI,IAAvB,EAA6B;AAC3B,UAAM,IAAI,KAAJ,CACF,UAAG,UAAH,4EADE,CAAN;AAGD;;AAED,MAAM,OAAO,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAH,GAAgB,CAAC,CAAC,KAAF,CAAQ,CAAR,CAA1C;AACA,MAAM,QAAQ,GAAG,UAAU,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAH,GAAgB,CAAC,CAAC,KAAF,CAAQ,CAAR,CAA3C;AACA,MAAM,QAAQ,GAAG,CAAC,CAAC,KAAF,CAAQ,CAAR,CAAjB;AAEA,MAAM,GAAG,GAAG,OAAO,CAAC,UAAR,CAAmB,CAAC,QAAD,EAAW,OAAX,EAAoB,QAApB,CAAnB,EAAkD,CAAC,CAAC,KAApD,CAAZ;AACA,MAAM,KAAK,GAAG,OAAO,CAAC,SAAR,CAAkB,GAAlB,CAAsB,GAAG,CAAC,MAA1B,EAAkC,EAAhD;AAEA,MAAM,WAAW,GAAG,IAAI,UAAJ,CAAe,IAAI,UAAJ,CAAe,CAAC,CAAC,KAAjB,EAAwB,MAAvC,CAApB;AACA,MAAM,WAAW,GAAG,IAAI,UAAJ,CAAe,IAAI,UAAJ,CAAe,CAAC,CAAC,KAAjB,EAAwB,MAAvC,CAApB;AAEA,EAAA,eAAe,CACX,GADW,EACN,WADM,EACO,CAAC,CAAC,KAAF,CAAQ,MADf,EACuB,GADvB,EAC4B,WAD5B,EACyC,CAAC,CAAC,KAAF,CAAQ,MADjD,EAEX,UAFW,EAEC,UAFD,EAEa,eAFb,EAE8B,MAF9B,EAEsC,wBAFtC,EAGX,cAAc,IAAI,CAHP,EAGU,KAHV,CAAf;AAKA,SAAO,GAAP;AACD;;AAED,OAAO,IAAM,iBAAiB,GAAiB;AAC7C,EAAA,UAAU,EAAE,YADiC;AAE7C,EAAA,WAAW,EAAE,MAFgC;AAG7C,EAAA,SAAS,EAAE,KAHkC;AAI7C,EAAA,UAAU,EAAE;AAJiC,CAAxC","sourcesContent":["/**\n * @license\n * Copyright 2019 Google LLC. All Rights Reserved.\n * Licensed under the Apache License, Version 2.0 (the \"License\");\n * you may not use this file except in compliance with the License.\n * You may obtain a copy of the License at\n *\n * http://www.apache.org/licenses/LICENSE-2.0\n *\n * Unless required by applicable law or agreed to in writing, software\n * distributed under the License is distributed on an \"AS IS\" BASIS,\n * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n * See the License for the specific language governing permissions and\n * limitations under the License.\n * =============================================================================\n */\n\nimport {_FusedMatMul, _FusedMatMulAttrs, _FusedMatMulInputs, KernelConfig, KernelFunc} from '@tensorflow/tfjs-core';\n\nimport {BackendWasm} from '../backend_wasm';\n\nimport {FusableActivation} from './types';\n\nlet wasmFusedMatMul:\n    (aId: number, aShape: Uint8Array, aShapeSize: number, bId: number,\n     bShape: Uint8Array, bShapeSize: number, transposeA: boolean,\n     transposeB: boolean, activation: number, biasId: number,\n     preluActivationWeightsId: number, leakyreluAlpha: number, outId: number) =>\n        void;\n\nfunction setup(backend: BackendWasm) {\n  wasmFusedMatMul = backend.wasm.cwrap(_FusedMatMul, null /* void */, [\n    'number',  // a_id\n    'array',   // a_shape\n    'number',  // a_shape.length\n    'number',  // b_id\n    'array',   // b_shape\n    'number',  // b_shape.length\n    'number',  // transpose_a\n    'number',  // transpose_b\n    'number',  // activation\n    'number',  // biasId\n    'number',  // preluActivationWeightsId\n    'number',  // leakyreluAlpha\n    'number'   // out_id\n  ]);\n}\n\nfunction fusedBatchMatMul(args: {\n  inputs: _FusedMatMulInputs,\n  backend: BackendWasm,\n  attrs: _FusedMatMulAttrs\n}) {\n  const {inputs, backend, attrs} = args;\n  const {a, b, bias, preluActivationWeights} = inputs;\n\n  if (a.dtype !== 'float32' || b.dtype !== 'float32') {\n    throw new Error(\n        `_FusedMatMul for non non-float32 tensors not yet supported.`);\n  }\n\n  const {transposeA, transposeB, activation, leakyreluAlpha} = attrs;\n  const aId = backend.dataIdMap.get(a.dataId).id;\n  const bId = backend.dataIdMap.get(b.dataId).id;\n\n  let biasId = 0;\n  if (bias != null) {\n    const biasData = backend.dataIdMap.get(bias.dataId);\n    if (biasData.shape.length !== 1) {\n      throw new Error(\n          `_FusedMatMul only supports rank-1 bias but got ` +\n          `rank ${biasData.shape.length}.`);\n    }\n    biasId = biasData.id;\n  }\n  const preluActivationWeightsId = preluActivationWeights == null ?\n      0 :\n      backend.dataIdMap.get(preluActivationWeights.dataId).id;\n  const fusedActivation =\n      FusableActivation[activation as {} as keyof typeof FusableActivation];\n  if (fusedActivation == null) {\n    throw new Error(\n        `${activation} activation not yet supported for FusedConv2D ` +\n        `in the wasm backend.`);\n  }\n\n  const leftDim = transposeA ? a.shape[2] : a.shape[1];\n  const rightDim = transposeB ? b.shape[1] : b.shape[2];\n  const batchDim = a.shape[0];\n\n  const out = backend.makeOutput([batchDim, leftDim, rightDim], a.dtype);\n  const outId = backend.dataIdMap.get(out.dataId).id;\n\n  const aShapeBytes = new Uint8Array(new Int32Array(a.shape).buffer);\n  const bShapeBytes = new Uint8Array(new Int32Array(b.shape).buffer);\n\n  wasmFusedMatMul(\n      aId, aShapeBytes, a.shape.length, bId, bShapeBytes, b.shape.length,\n      transposeA, transposeB, fusedActivation, biasId, preluActivationWeightsId,\n      leakyreluAlpha || 0, outId);\n\n  return out;\n}\n\nexport const fusedMatMulConfig: KernelConfig = {\n  kernelName: _FusedMatMul,\n  backendName: 'wasm',\n  setupFunc: setup,\n  kernelFunc: fusedBatchMatMul as {} as KernelFunc\n};\n"],"sourceRoot":""},"metadata":{},"sourceType":"module"}